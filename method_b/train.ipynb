{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch\n",
    "# ! pip install torchtext==0.6.0\n",
    "# ! pip install scapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from classifier import NLIModel\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import GloVe, Vocab\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu118\n",
      "0.6.0\n",
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import torchtext\n",
    "print(torchtext.__version__)\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deifine Field For Tokenise and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define how to preprocess the text data\n",
    "TEXT = data.Field(lower=True, sequential=True, tokenize=lambda text: [token.text for token in spacy_en.tokenizer(text)])\n",
    "# Define how to process the labels\n",
    "LABEL = data.Field(sequential=False, use_vocab=False, unk_token=None)\n",
    "\n",
    "fields = [('premise', TEXT), ('hypothesis', TEXT), ('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['however', ',', 'fort', 'charles', 'was', 'rebuilt', 'as', 'a', 'military', 'and', 'naval', 'garrison', ',', 'and', 'it', 'protected', 'jamaica', 'and', 'much', 'of', 'the', 'english', 'caribbean', 'for', '250', 'years', 'until', 'the', 'advent', 'of', 'steamships', 'and', 'yet', 'another', 'earthquake', 'in', '1907', 'saw', 'its', 'decline', '.'], 'hypothesis': ['fort', 'charles', 'was', 'rebuilt', 'as', 'an', 'amusement', 'park', 'for', 'the', 'locals', '.'], 'label': '0'}\n",
      "{'premise': ['mon', 'dieu', '!'], 'hypothesis': ['this', 'person', 'is', 'speaking', 'english', '.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = data.TabularDataset.splits(\n",
    "        path='./',  # Directory of your CSV files\n",
    "        train='train.csv', validation='dev.csv',\n",
    "        format='csv',\n",
    "        fields=fields,\n",
    "        skip_header=True  # If your CSV has a header\n",
    ")\n",
    "\n",
    "# Build the vocabulary only for the TEXT field from the training set\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.840B.300d\")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(vars(train_data.examples[0]))  # Print the first training example\n",
    "print(vars(validation_data.examples[0]))  # Print the first validation example\n",
    "\n",
    "# print(\"Premise:\", ' '.join(train_data.premise))\n",
    "# print(\"Hypothesis:\", ' '.join(train_data.hypothesis))\n",
    "# print(\"Label:\", train_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([35538, 300])\n"
     ]
    }
   ],
   "source": [
    "print(type(TEXT.vocab.vectors))  # Expected: <class 'torch.Tensor'>\n",
    "print(TEXT.vocab.vectors.size())  # Expected output: torch.Size([vocab_size, embedding_dim]) --> torch.Size([35538, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model, Loss Function, and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dropout=0.5, activation='leakyrelu', hidden_dim=600, fc_dim=600, out_dim=2, embed_size=35538, embed_dim=300, encoder_type='HBMP', layers=1, cells=2, word_embedding='glove.840B.300d', epochs=20, batch_size=32, optimizer='adam', learning_rate=0.0005, lr_patience=1, lr_reduction_factor=0.2, weight_decay=0, early_stopping_patience=3, save_path='results', gpu='cpu')\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "original_argv = sys.argv\n",
    "sys.argv = ['']\n",
    "\n",
    "parser = ArgumentParser(description='Helsinki NLI')\n",
    "\n",
    "config = parser.parse_args()\n",
    "config.dropout =  0.5\n",
    "config.activation = 'leakyrelu'\n",
    "config.hidden_dim = 600\n",
    "config.fc_dim = 600\n",
    "config.out_dim = len(LABEL.vocab)\n",
    "config.embed_size = len(TEXT.vocab)\n",
    "config.embed_dim = TEXT.vocab.vectors.size(1)\n",
    "config.encoder_type = 'HBMP'\n",
    "config.layers = 1\n",
    "config.cells = config.layers * 2\n",
    "config.word_embedding = 'glove.840B.300d'\n",
    "config.epochs = 20\n",
    "config.batch_size = 32\n",
    "config.optimizer = 'adam'\n",
    "config.learning_rate = 0.0005\n",
    "config.lr_patience = 1\n",
    "# config.lr_decay = 0.99\n",
    "config.lr_reduction_factor = 0.2\n",
    "config.weight_decay = 0\n",
    "config.early_stopping_patience = 3\n",
    "config.save_path = 'results'\n",
    "config.gpu = 'cpu'\n",
    "# config.seed = 1234\n",
    "\n",
    "# Restore the original sys.argv\n",
    "sys.argv = original_argv\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter = data.BucketIterator.splits(\n",
    "        (train_data, validation_data),\n",
    "        batch_size=config.batch_size,\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.premise),\n",
    "        device=device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: in 2000 , iolta collected $ 16 .\n",
      "Hypothesis: iolta collected thousands of dollars that year . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Getting a single batch from the iterator\n",
    "for batch in train_iter:\n",
    "    # Assuming 'premise' and 'hypothesis' are your input fields and 'label' is your target field\n",
    "    premises = batch.premise\n",
    "    hypotheses = batch.hypothesis\n",
    "\n",
    "    for i in range(premises.shape[1]):  # Loop over batch size\n",
    "        # Convert indices back to strings (for textual data fields)\n",
    "        # Note: This step assumes that you have the TEXT field build vocab\n",
    "        premise = ' '.join([TEXT.vocab.itos[ind] for ind in premises[:, i].tolist()])\n",
    "        hypothesis = ' '.join([TEXT.vocab.itos[ind] for ind in hypotheses[:, i].tolist()])\n",
    "        # print(batch.label)\n",
    "        label = batch.label[i].item()\n",
    "\n",
    "        print(\"Premise:\", premise)\n",
    "        print(\"Hypothesis:\", hypothesis)\n",
    "        print(\"Label:\", label)\n",
    "        \n",
    "        # Break after the first batch to only see one batch\n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Anaconda\\envs\\NLU\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "\n",
      "NLIModel(\n",
      "  (sentence_embedding): SentenceEmbedding(\n",
      "    (word_embedding): Embedding(35538, 300)\n",
      "    (encoder): HBMP(\n",
      "      (max_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "      (rnn1): LSTM(300, 600, dropout=0.5, bidirectional=True)\n",
      "      (rnn2): LSTM(300, 600, dropout=0.5, bidirectional=True)\n",
      "      (rnn3): LSTM(300, 600, dropout=0.5, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): FCClassifier(\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "    (mlp): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=14400, out_features=600, bias=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=600, out_features=600, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=600, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Parameters: 32652602\n"
     ]
    }
   ],
   "source": [
    "model = NLIModel(config).to(device)\n",
    "\n",
    "# if config.word_embedding:\n",
    "#     model.sentence_embedding.word_embedding.weight.data = TEXT.vocab.vectors\n",
    "#       model.cuda(device=config.gpu)\n",
    "\n",
    "# Print the model\n",
    "print('Model:\\n')\n",
    "print(model)\n",
    "print('\\n')\n",
    "params = sum([p.numel() for p in model.parameters()])\n",
    "print('Parameters: {}'.format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name):\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except OSError as ex:\n",
    "        if ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "            # ignore existing directory\n",
    "            pass\n",
    "        else:\n",
    "            # a different error happened\n",
    "            raise\n",
    "\n",
    "make_dirs(config.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started...\n",
      "\n",
      "\n",
      "Epoch: 01/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  61.85% - Accuracy:  59.94%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 55.68% - Dev accuracy: 70.64%\n",
      "\n",
      "Epoch: 02/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  53.35% - Accuracy:  72.19%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 53.7% - Dev accuracy: 71.26%\n",
      "\n",
      "Epoch: 03/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  46.40% - Accuracy:  77.90%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 54.31% - Dev accuracy: 72.17%\n",
      "\n",
      "Epoch: 04/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  36.45% - Accuracy:  84.32%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 62.01% - Dev accuracy: 70.89%\n",
      "\n",
      "Epoch: 05/20\t(Learning rate: 0.0001)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  18.10% - Accuracy:  92.75%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 77.89% - Dev accuracy: 71.07%\n",
      "\n",
      "Epoch: 06/20\t(Learning rate: 0.0001)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  10.26% - Accuracy:  96.00%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 100.66% - Dev accuracy: 70.33%\n",
      "\n",
      "Early stopping\n",
      "\n",
      "Training completed after 6 epocs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate,)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                            'min',\n",
    "                                            factor=config.lr_reduction_factor,\n",
    "                                            patience=config.lr_patience,\n",
    "                                            min_lr=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "best_dev_acc = -1\n",
    "dev_accuracies = []\n",
    "best_dev_loss = 1\n",
    "\n",
    "early_stopping = 0\n",
    "stop_training = False\n",
    "train_iter.repeat = False\n",
    "\n",
    "\n",
    "print('\\nTraining started...\\n')\n",
    "# Training and Evaluation\n",
    "best_acc = 0.0\n",
    "for epoch in range(config.epochs):  # Adjust the number of epochs\n",
    "\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    train_accuracies = []\n",
    "    all_losses = []\n",
    "\n",
    "    print('\\nEpoch: {:>02.0f}/{:<02.0f}'.format(epoch+1, config.epochs), end='\\t')\n",
    "    print('(Learning rate: {})'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch data to the correct device\n",
    "        batch.premise, batch.hypothesis, batch.label = batch.premise.to(device), batch.hypothesis.to(device), batch.label.to(device)\n",
    "        predictions = model(batch)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        n_correct += (torch.max(predictions, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "        n_total += batch.batch_size\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "        train_accuracies.append(train_acc.item())\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        # Backpropagate and update the learning rate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # For accuracy calculation\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        print('Progress: {:3.0f}% - Batch: {:>4.0f}/{:>4.0f} - Loss: {:6.2f}% - Accuracy: {:6.2f}%'.format(\n",
    "            100. * (1+batch_idx) / len(train_iter),\n",
    "            1+batch_idx, len(train_iter),\n",
    "            round(100. * np.mean(all_losses), 2),\n",
    "            round(np.mean(train_accuracies), 2)), end='\\r')\n",
    "\n",
    "    \n",
    "    print('\\nEvaluation started...\\n')\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    n_dev_correct = 0\n",
    "    dev_loss = 0\n",
    "    dev_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "            answer = model(dev_batch)\n",
    "\n",
    "            n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == \\\n",
    "                dev_batch.label.data).sum()\n",
    "            \n",
    "            dev_loss = criterion(answer, dev_batch.label)\n",
    "            dev_losses.append(dev_loss.item())\n",
    "        \n",
    "        dev_acc = 100. * n_dev_correct / len(validation_data)\n",
    "        dev_acc=dev_acc.item()\n",
    "        dev_accuracies.append(dev_acc)\n",
    "\n",
    "        print('\\nDev loss: {}% - Dev accuracy: {}%'.format(round(100.*np.mean(dev_losses), 2), round(dev_acc, 2)))\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "\n",
    "            best_dev_acc = dev_acc\n",
    "            best_dev_epoch = 1+epoch\n",
    "            snapshot_prefix = os.path.join(config.save_path, 'best')\n",
    "            dev_snapshot_path = snapshot_prefix + \\\n",
    "                '_{}_{}D_devacc_{}_epoch_{}.pt'.format(config.encoder_type, config.hidden_dim, round(dev_acc, 2), 1+epoch)\n",
    "        \n",
    "            # save model, delete previous snapshot\n",
    "            torch.save(model, dev_snapshot_path)\n",
    "            for f in glob.glob(snapshot_prefix + '*'):\n",
    "                if f != dev_snapshot_path:\n",
    "                    os.remove(f)\n",
    "\n",
    "\n",
    "        # Check for early stopping\n",
    "        if np.mean(dev_losses) < best_dev_loss:\n",
    "            best_dev_loss = np.mean(dev_losses)\n",
    "        else:\n",
    "            early_stopping += 1\n",
    "\n",
    "        if early_stopping > config.early_stopping_patience and config.optimizer != 'sgd':\n",
    "            stop_training = True\n",
    "            print('\\nEarly stopping')\n",
    "\n",
    "        if config.optimizer == 'sgd' and optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "            stop_training = True\n",
    "            print('\\nEarly stopping')\n",
    "            \n",
    "        # Update learning rate\n",
    "        scheduler.step(round(np.mean(dev_losses), 2))\n",
    "        dev_losses = []\n",
    "\n",
    "\n",
    "    # If training has completed, calculate the test scores\n",
    "    if stop_training == True or (1+epoch == config.epochs and 1+batch_idx == len(train_iter)):\n",
    "        print('\\nTraining completed after {} epocs.\\n'.format(1+epoch))\n",
    "\n",
    "\n",
    "        #Save the final model\n",
    "        final_snapshot_prefix = os.path.join(config.save_path, 'final')\n",
    "        final_snapshot_path = final_snapshot_prefix + \\\n",
    "        '_{}_{}D.pt'.format(config.encoder_type, config.hidden_dim)\n",
    "        torch.save(model, final_snapshot_path)\n",
    "        for f in glob.glob(final_snapshot_prefix + '*'):\n",
    "            if f != final_snapshot_path:\n",
    "                os.remove(f)\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
