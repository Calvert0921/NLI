{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch\n",
    "! pip install torchtext==0.6.0\n",
    "! pip install scapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from classifier import NLIModel\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import GloVe, Vocab\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cpu\n",
      "0.6.0\n",
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "import torchtext\n",
    "print(torchtext.__version__)\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deifine Field For Tokenise and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define how to preprocess the text data\n",
    "TEXT = data.Field(lower=True, sequential=True, tokenize=lambda text: [token.text for token in spacy_en.tokenizer(text)])\n",
    "# Define how to process the labels\n",
    "LABEL = data.Field(sequential=False, use_vocab=False, unk_token=None)\n",
    "\n",
    "fields = [('premise', TEXT), ('hypothesis', TEXT), ('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x00000234D25BBD10>\n",
      "<torchtext.data.example.Example object at 0x00000234DB34A8D0>\n",
      "{'premise': ['however', ',', 'fort', 'charles', 'was', 'rebuilt', 'as', 'a', 'military', 'and', 'naval', 'garrison', ',', 'and', 'it', 'protected', 'jamaica', 'and', 'much', 'of', 'the', 'english', 'caribbean', 'for', '250', 'years', 'until', 'the', 'advent', 'of', 'steamships', 'and', 'yet', 'another', 'earthquake', 'in', '1907', 'saw', 'its', 'decline', '.'], 'hypothesis': ['fort', 'charles', 'was', 'rebuilt', 'as', 'an', 'amusement', 'park', 'for', 'the', 'locals', '.'], 'label': '0'}\n",
      "{'premise': ['mon', 'dieu', '!'], 'hypothesis': ['this', 'person', 'is', 'speaking', 'english', '.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = data.TabularDataset.splits(\n",
    "        path='./',  # Directory of your CSV files\n",
    "        train='train.csv', validation='dev.csv',\n",
    "        format='csv',\n",
    "        fields=fields,\n",
    "        skip_header=True  # If your CSV has a header\n",
    ")\n",
    "\n",
    "# Build the vocabulary only for the TEXT field from the training set\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.840B.300d\")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(vars(train_data.examples[0]))  # Print the first training example\n",
    "print(vars(validation_data.examples[0]))  # Print the first validation example\n",
    "\n",
    "# print(\"Premise:\", ' '.join(train_data.premise))\n",
    "# print(\"Hypothesis:\", ' '.join(train_data.hypothesis))\n",
    "# print(\"Label:\", train_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([35538, 300])\n"
     ]
    }
   ],
   "source": [
    "print(type(TEXT.vocab.vectors))  # Expected: <class 'torch.Tensor'>\n",
    "print(TEXT.vocab.vectors.size())  # Expected output: torch.Size([vocab_size, embedding_dim]) --> torch.Size([35538, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model, Loss Function, and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dropout=0.5, activation='leakyrelu', hidden_dim=600, fc_dim=600, out_dim=2, embed_size=35538, embed_dim=300, encoder_type='HBMP', layers=1, cells=2, word_embedding='glove.840B.300d', epochs=20, batch_size=32, optimizer='adam', learning_rate=0.0005, lr_patience=1, lr_decay=0.99, lr_reduction_factor=0.2, weight_decay=0, early_stopping_patience=3, save_path='results', gpu='cpu')\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "original_argv = sys.argv\n",
    "sys.argv = ['']\n",
    "\n",
    "parser = ArgumentParser(description='Helsinki NLI')\n",
    "\n",
    "config = parser.parse_args()\n",
    "config.dropout =  0.5\n",
    "config.activation = 'leakyrelu'\n",
    "config.hidden_dim = 600\n",
    "config.fc_dim = 600\n",
    "config.out_dim = len(LABEL.vocab)\n",
    "config.embed_size = len(TEXT.vocab)\n",
    "config.embed_dim = TEXT.vocab.vectors.size(1)\n",
    "config.encoder_type = 'HBMP'\n",
    "config.layers = 1\n",
    "config.cells = config.layers * 2\n",
    "config.word_embedding = 'glove.840B.300d'\n",
    "config.epochs = 20\n",
    "config.batch_size = 32\n",
    "config.optimizer = 'adam'\n",
    "config.learning_rate = 0.0005\n",
    "config.lr_patience = 1\n",
    "# config.lr_decay = 0.99\n",
    "config.lr_reduction_factor = 0.2\n",
    "config.weight_decay = 0\n",
    "config.early_stopping_patience = 3\n",
    "config.save_path = 'results'\n",
    "config.gpu = 'cpu'\n",
    "# config.seed = 1234\n",
    "\n",
    "# Restore the original sys.argv\n",
    "sys.argv = original_argv\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter = data.BucketIterator.splits(\n",
    "        (train_data, validation_data),\n",
    "        batch_size=config.batch_size,\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.premise),\n",
    "        device=device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: from the ticket office you enter the temple complex through a colossal pylon , one of the most recent structures at the site and the largest constructed anywhere in egypt during the ptolemaic period .\n",
      "Hypothesis: the large pylon at the entrance of the temple complex was erected in honor of isis .   <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Getting a single batch from the iterator\n",
    "for batch in train_iter:\n",
    "    # Assuming 'premise' and 'hypothesis' are your input fields and 'label' is your target field\n",
    "    premises = batch.premise\n",
    "    hypotheses = batch.hypothesis\n",
    "\n",
    "    for i in range(premises.shape[1]):  # Loop over batch size\n",
    "        # Convert indices back to strings (for textual data fields)\n",
    "        # Note: This step assumes that you have the TEXT field build vocab\n",
    "        premise = ' '.join([TEXT.vocab.itos[ind] for ind in premises[:, i].tolist()])\n",
    "        hypothesis = ' '.join([TEXT.vocab.itos[ind] for ind in hypotheses[:, i].tolist()])\n",
    "        # print(batch.label)\n",
    "        label = batch.label[i].item()\n",
    "\n",
    "        print(\"Premise:\", premise)\n",
    "        print(\"Hypothesis:\", hypothesis)\n",
    "        print(\"Label:\", label)\n",
    "        \n",
    "        # Break after the first batch to only see one batch\n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "\n",
      "NLIModel(\n",
      "  (sentence_embedding): SentenceEmbedding(\n",
      "    (word_embedding): Embedding(35538, 300)\n",
      "    (encoder): HBMP(\n",
      "      (max_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "      (rnn1): LSTM(300, 600, dropout=0.5, bidirectional=True)\n",
      "      (rnn2): LSTM(300, 600, dropout=0.5, bidirectional=True)\n",
      "      (rnn3): LSTM(300, 600, dropout=0.5, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): FCClassifier(\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "    (mlp): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=14400, out_features=600, bias=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=600, out_features=600, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=600, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Parameters: 32652602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = NLIModel(config).to(device)\n",
    "\n",
    "# if config.word_embedding:\n",
    "#     model.sentence_embedding.word_embedding.weight.data = TEXT.vocab.vectors\n",
    "#       model.cuda(device=config.gpu)\n",
    "\n",
    "# Print the model\n",
    "print('Model:\\n')\n",
    "print(model)\n",
    "print('\\n')\n",
    "params = sum([p.numel() for p in model.parameters()])\n",
    "print('Parameters: {}'.format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name):\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except OSError as ex:\n",
    "        if ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "            # ignore existing directory\n",
    "            pass\n",
    "        else:\n",
    "            # a different error happened\n",
    "            raise\n",
    "\n",
    "make_dirs(config.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started...\n",
      "\n",
      "\n",
      "Epoch: 01/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  61.99% - Accuracy:  59.33%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 55.94% - Dev accuracy: 70.39%\n",
      "\n",
      "Epoch: 02/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  54.20% - Accuracy:  71.40%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 53.69% - Dev accuracy: 71.38%\n",
      "\n",
      "Epoch: 03/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  46.76% - Accuracy:  77.23%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 56.67% - Dev accuracy: 69.44%\n",
      "\n",
      "Epoch: 04/20\t(Learning rate: 0.0005)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  37.45% - Accuracy:  83.41%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 62.71% - Dev accuracy: 70.91%\n",
      "\n",
      "Epoch: 05/20\t(Learning rate: 0.0001)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  17.66% - Accuracy:  92.86%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 85.76% - Dev accuracy: 70.77%\n",
      "\n",
      "Epoch: 06/20\t(Learning rate: 0.0001)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:  10.25% - Accuracy:  96.15%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 105.07% - Dev accuracy: 70.68%\n",
      "\n",
      "Early stopping\n",
      "\n",
      "Training completed after 6 epocs.\n",
      "\n",
      "\n",
      "Epoch: 07/20\t(Learning rate: 2e-05)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:   6.08% - Accuracy:  97.98%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 124.74% - Dev accuracy: 70.51%\n",
      "\n",
      "Early stopping\n",
      "\n",
      "Training completed after 7 epocs.\n",
      "\n",
      "\n",
      "Epoch: 08/20\t(Learning rate: 2e-05)\n",
      "Progress: 100% - Batch:  842/ 842 - Loss:   4.91% - Accuracy:  98.28%\n",
      "Evaluation started...\n",
      "\n",
      "\n",
      "Dev loss: 136.77% - Dev accuracy: 70.16%\n",
      "\n",
      "Early stopping\n",
      "\n",
      "Training completed after 8 epocs.\n",
      "\n",
      "\n",
      "Epoch: 09/20\t(Learning rate: 1e-05)\n",
      "Progress:  85% - Batch:  715/ 842 - Loss:   4.50% - Accuracy:  98.41%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Move batch data to the correct device\u001b[39;00m\n\u001b[0;32m     40\u001b[0m batch\u001b[38;5;241m.\u001b[39mpremise, batch\u001b[38;5;241m.\u001b[39mhypothesis, batch\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpremise\u001b[38;5;241m.\u001b[39mto(device), batch\u001b[38;5;241m.\u001b[39mhypothesis\u001b[38;5;241m.\u001b[39mto(device), batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 41\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m     44\u001b[0m n_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmax(predictions, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m==\u001b[39m batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Desktop\\a\\NLU_1\\classifier.py:58\u001b[0m, in \u001b[0;36mNLIModel.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m     57\u001b[0m     prem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_embedding(batch\u001b[38;5;241m.\u001b[39mpremise)\n\u001b[1;32m---> 58\u001b[0m     hypo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypothesis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(prem, hypo)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Desktop\\a\\NLU_1\\embeddings.py:20\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[1;34m(self, input_sentence)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_sentence):\n\u001b[0;32m     19\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding(input_sentence)\n\u001b[1;32m---> 20\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Desktop\\a\\NLU_1\\embeddings.py:118\u001b[0m, in \u001b[0;36mHBMP.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    115\u001b[0m out2, (ht2, ct2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn2(inputs, (ht1, ct1))\n\u001b[0;32m    116\u001b[0m emb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool(out2\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m out3, (ht3, ct3) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mht2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m emb3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool(out3\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    121\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([emb1, emb2, emb3], \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate,)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                            'min',\n",
    "                                            factor=config.lr_reduction_factor,\n",
    "                                            patience=config.lr_patience,\n",
    "                                            min_lr=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "best_dev_acc = -1\n",
    "dev_accuracies = []\n",
    "best_dev_loss = 1\n",
    "\n",
    "early_stopping = 0\n",
    "stop_training = False\n",
    "train_iter.repeat = False\n",
    "\n",
    "\n",
    "print('\\nTraining started...\\n')\n",
    "# Training and Evaluation\n",
    "best_acc = 0.0\n",
    "for epoch in range(config.epochs):  # Adjust the number of epochs\n",
    "\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    train_accuracies = []\n",
    "    all_losses = []\n",
    "\n",
    "    print('\\nEpoch: {:>02.0f}/{:<02.0f}'.format(epoch+1, config.epochs), end='\\t')\n",
    "    print('(Learning rate: {})'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch data to the correct device\n",
    "        batch.premise, batch.hypothesis, batch.label = batch.premise.to(device), batch.hypothesis.to(device), batch.label.to(device)\n",
    "        predictions = model(batch)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        n_correct += (torch.max(predictions, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "        n_total += batch.batch_size\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "        train_accuracies.append(train_acc.item())\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        # Backpropagate and update the learning rate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # For accuracy calculation\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        print('Progress: {:3.0f}% - Batch: {:>4.0f}/{:>4.0f} - Loss: {:6.2f}% - Accuracy: {:6.2f}%'.format(\n",
    "            100. * (1+batch_idx) / len(train_iter),\n",
    "            1+batch_idx, len(train_iter),\n",
    "            round(100. * np.mean(all_losses), 2),\n",
    "            round(np.mean(train_accuracies), 2)), end='\\r')\n",
    "\n",
    "    \n",
    "    print('\\nEvaluation started...\\n')\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    n_dev_correct = 0\n",
    "    dev_loss = 0\n",
    "    dev_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "            answer = model(dev_batch)\n",
    "\n",
    "            n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == \\\n",
    "                dev_batch.label.data).sum()\n",
    "            \n",
    "            dev_loss = criterion(answer, dev_batch.label)\n",
    "            dev_losses.append(dev_loss.item())\n",
    "        \n",
    "        dev_acc = 100. * n_dev_correct / len(validation_data)\n",
    "        dev_acc=dev_acc.item()\n",
    "        dev_accuracies.append(dev_acc)\n",
    "\n",
    "        print('\\nDev loss: {}% - Dev accuracy: {}%'.format(round(100.*np.mean(dev_losses), 2), round(dev_acc, 2)))\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "\n",
    "            best_dev_acc = dev_acc\n",
    "            best_dev_epoch = 1+epoch\n",
    "            snapshot_prefix = os.path.join(config.save_path, 'best')\n",
    "            dev_snapshot_path = snapshot_prefix + \\\n",
    "                '_{}_{}D_devacc_{}_epoch_{}.pt'.format(config.encoder_type, config.hidden_dim, round(dev_acc, 2), 1+epoch)\n",
    "        \n",
    "            # save model, delete previous snapshot\n",
    "            torch.save(model, dev_snapshot_path)\n",
    "            for f in glob.glob(snapshot_prefix + '*'):\n",
    "                if f != dev_snapshot_path:\n",
    "                    os.remove(f)\n",
    "\n",
    "\n",
    "        # Check for early stopping\n",
    "        if np.mean(dev_losses) < best_dev_loss:\n",
    "            best_dev_loss = np.mean(dev_losses)\n",
    "        else:\n",
    "            early_stopping += 1\n",
    "\n",
    "        if early_stopping > config.early_stopping_patience and config.optimizer != 'sgd':\n",
    "            stop_training = True\n",
    "            print('\\nEarly stopping')\n",
    "\n",
    "        if config.optimizer == 'sgd' and optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "            stop_training = True\n",
    "            print('\\nEarly stopping')\n",
    "            \n",
    "        # Update learning rate\n",
    "        scheduler.step(round(np.mean(dev_losses), 2))\n",
    "        dev_losses = []\n",
    "\n",
    "\n",
    "    # If training has completed, calculate the test scores\n",
    "    if stop_training == True or (1+epoch == config.epochs and 1+batch_idx == len(train_iter)):\n",
    "        print('\\nTraining completed after {} epocs.\\n'.format(1+epoch))\n",
    "\n",
    "\n",
    "        #Save the final model\n",
    "        final_snapshot_prefix = os.path.join(config.save_path, 'final')\n",
    "        final_snapshot_path = final_snapshot_prefix + \\\n",
    "        '_{}_{}D.pt'.format(config.encoder_type, config.hidden_dim)\n",
    "        torch.save(model, final_snapshot_path)\n",
    "        for f in glob.glob(final_snapshot_prefix + '*'):\n",
    "            if f != final_snapshot_path:\n",
    "                os.remove(f)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNWANTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config: ['--f=c:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-12832L9wxdsH5Cnc8.json']\n",
      "\n",
      "Namespace(dropout=0.1, activation='leakyrelu', hidden_dim=600, fc_dim=600, out_dim=2, embed_size=35538, embed_dim=300, encoder_type='HBMP', layers=2, cells=4, word_embedding='glove.840B.300d', epochs=1, batch_size=64, optimizer='adam', learning_rate=0.0005, lr_patience=1, lr_decay=0.99, lr_reduction_factor=0.2, weight_decay=0, early_stopping_patience=3, save_path='results', gpu='cpu')\n",
      "Model:\n",
      "\n",
      "NLIModel(\n",
      "  (sentence_embedding): SentenceEmbedding(\n",
      "    (word_embedding): Embedding(35538, 300)\n",
      "    (encoder): HBMP(\n",
      "      (max_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "      (rnn1): LSTM(300, 600, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "      (rnn2): LSTM(300, 600, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "      (rnn3): LSTM(300, 600, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): FCClassifier(\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "    (mlp): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=14400, out_features=600, bias=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=600, out_features=600, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=600, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Parameters: 58601402\n"
     ]
    }
   ],
   "source": [
    "# # Loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizer\n",
    "# if config.optimizer == 'adadelta':\n",
    "#     optim_algorithm = optim.Adadelta\n",
    "# elif config.optimizer == 'adagrad':\n",
    "#     optim_algorithm = optim.Adagrad\n",
    "# elif config.optimizer == 'adam':\n",
    "#     optim_algorithm = optim.Adam\n",
    "# elif config.optimizer == 'adamax':\n",
    "#     optim_algorithm = optim.Adamax\n",
    "# elif config.optimizer == 'asgd':\n",
    "#     optim_algorithm = optim.ASGD\n",
    "# elif config.optimizer == 'rmsprop':\n",
    "#     optim_algorithm = optim.RMSprop\n",
    "# elif config.optimizer == 'rprop':\n",
    "#     optim_algorithm = optim.Rprop\n",
    "# elif config.optimizer == 'sgd':\n",
    "#     optim_algorithm = optim.SGD\n",
    "# else:\n",
    "#     raise Exception('Unknown optimization optimizer: \"%s\"' % config.optimizer)\n",
    "\n",
    "# optimizer = optim_algorithm(model.parameters(),\n",
    "#                             lr=config.learning_rate,\n",
    "#                             weight_decay=config.weight_decay)\n",
    "\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "#                                             'min',\n",
    "#                                             factor=config.lr_reduction_factor,\n",
    "#                                             patience=config.lr_patience,\n",
    "#                                             min_lr=1e-5)\n",
    "\n",
    "# iterations = 0\n",
    "# best_dev_acc = -1\n",
    "# dev_accuracies = []\n",
    "# best_dev_loss = 1\n",
    "# early_stopping = 0\n",
    "# stop_training = False\n",
    "# train_iter.repeat = False\n",
    "# make_dirs(config.save_path)\n",
    "\n",
    "# # Print parameters and config\n",
    "# print('\\nConfig: {}\\n'.format(sys.argv[1:]))\n",
    "# print(config)\n",
    "\n",
    "# # Print the model\n",
    "# print('Model:\\n')\n",
    "# print(model)\n",
    "# print('\\n')\n",
    "# params = sum([p.numel() for p in model.parameters()])\n",
    "# print('Parameters: {}'.format(params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started...\n",
      "\n",
      "\n",
      "Epoch: 01/10\t(Learning rate: 0.0005)\n",
      "Progress:   5% - Batch:   21/421  - Loss:  69.51% - Accuracy:  48.94%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m all_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Backpropagate and update the learning rate\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{:3.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m - Batch: \u001b[39m\u001b[38;5;132;01m{:>4.0f}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{:<4.0f}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{:6.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m - Accuracy: \u001b[39m\u001b[38;5;132;01m{:6.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mbatch_idx) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_iter),\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mbatch_idx, \u001b[38;5;28mlen\u001b[39m(train_iter),\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(all_losses), \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(train_accuracies), \u001b[38;5;241m2\u001b[39m)), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print('\\nTraining started...\\n')\n",
    "\n",
    "# # Train for the number of epochs specified\n",
    "# for epoch in range(config.epochs):\n",
    "#     if stop_training == True:\n",
    "#         break\n",
    "\n",
    "#     train_iter.init_epoch()\n",
    "#     n_correct = 0\n",
    "#     n_total = 0\n",
    "#     all_losses = []\n",
    "#     train_accuracies = []\n",
    "#     all_losses = []\n",
    "\n",
    "#     optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * config.lr_decay if epoch>0\\\n",
    "#     and config.optimizer == 'sgd' else optimizer.param_groups[0]['lr']\n",
    "#     print('\\nEpoch: {:>02.0f}/{:>02.0f}'.format(epoch+1, config.epochs), end='\\t')\n",
    "#     print('(Learning rate: {})'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "#     for batch_idx, batch in enumerate(train_iter):\n",
    "\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         iterations += 1\n",
    "#         answer = model(batch)\n",
    "#         # sys.exit()\n",
    "#         # Calculate accuracy\n",
    "\n",
    "#         n_correct += (torch.max(answer, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "#         n_total += batch.batch_size\n",
    "#         train_acc = 100. * n_correct/n_total\n",
    "#         train_accuracies.append(train_acc.item())\n",
    "\n",
    "#         # Calculate loss\n",
    "#         loss = criterion(answer, batch.label)\n",
    "#         all_losses.append(loss.item())\n",
    "\n",
    "#         # Backpropagate and update the learning rate\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         print('Progress: {:3.0f}% - Batch: {:>4.0f}/{:<4.0f} - Loss: {:6.2f}% - Accuracy: {:6.2f}%'.format(\n",
    "#             100. * (1+batch_idx) / len(train_iter),\n",
    "#             1+batch_idx, len(train_iter),\n",
    "#             round(100. * np.mean(all_losses), 2),\n",
    "#             round(np.mean(train_accuracies), 2)), end='\\r')\n",
    "        \n",
    "        \n",
    "#         # Evaluate performance\n",
    "#         # if iterations % config.dev_every == 0:\n",
    "#         if 1+batch_idx == len(train_iter):\n",
    "#             # Switch model to evaluation mode\n",
    "#             model.eval()\n",
    "#             dev_iter.init_epoch()\n",
    "\n",
    "#             # Calculate Accuracy\n",
    "#             n_dev_correct = 0\n",
    "#             dev_loss = 0\n",
    "#             dev_losses = []\n",
    "\n",
    "#             for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "#                 answer = model(dev_batch)\n",
    "#                 n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == \\\n",
    "#                     dev_batch.label.data).sum()\n",
    "#                 dev_loss = criterion(answer, dev_batch.label)\n",
    "#                 dev_losses.append(dev_loss.item())\n",
    "\n",
    "#             dev_acc = 100. * n_dev_correct / len(dev)\n",
    "#             dev_acc=dev_acc.item()\n",
    "#             dev_accuracies.append(dev_acc)\n",
    "\n",
    "#             print('\\nDev loss: {}% - Dev accuracy: {}%'.format(round(100.*np.mean(dev_losses), 2), round(dev_acc, 2)))\n",
    "\n",
    "#             # Update validation best accuracy if it is better than\n",
    "#             # already stored\n",
    "#             if dev_acc > best_dev_acc:\n",
    "\n",
    "#                 best_dev_acc = dev_acc\n",
    "#                 best_dev_epoch = 1+epoch\n",
    "#                 snapshot_prefix = os.path.join(config.save_path, 'best')\n",
    "#                 dev_snapshot_path = snapshot_prefix + \\\n",
    "#                     '_{}_{}D_devacc_{}_epoch_{}.pt'.format(config.encoder_type, config.hidden_dim, round(dev_acc, 2), 1+epoch)\n",
    "\n",
    "#                 # save model, delete previous snapshot\n",
    "#                 torch.save(model, dev_snapshot_path)\n",
    "#                 for f in glob.glob(snapshot_prefix + '*'):\n",
    "#                     if f != dev_snapshot_path:\n",
    "#                         os.remove(f)\n",
    "\n",
    "#             # Check for early stopping\n",
    "#             if np.mean(dev_losses) < best_dev_loss:\n",
    "#                 best_dev_loss = np.mean(dev_losses)\n",
    "#             else:\n",
    "#                 early_stopping += 1\n",
    "\n",
    "#             if early_stopping > config.early_stopping_patience and config.optimizer != 'sgd':\n",
    "#                 stop_training = True\n",
    "#                 print('\\nEarly stopping')\n",
    "\n",
    "#             if config.optimizer == 'sgd' and optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "#                 stop_training = True\n",
    "#                 print('\\nEarly stopping')\n",
    "\n",
    "#             # Update learning rate\n",
    "#             scheduler.step(round(np.mean(dev_losses), 2))\n",
    "#             dev_losses = []\n",
    "\n",
    "\n",
    "#         # If training has completed, calculate the test scores\n",
    "#         if stop_training == True or (1+epoch == config.epochs and 1+batch_idx == len(train_iter)):\n",
    "#             print('\\nTraining completed after {} epocs.\\n'.format(1+epoch))\n",
    "\n",
    "\n",
    "#             #Save the final model\n",
    "#             final_snapshot_prefix = os.path.join(config.save_path, 'final')\n",
    "#             final_snapshot_path = final_snapshot_prefix + \\\n",
    "#             '_{}_{}D.pt'.format(config.encoder_type, config.hidden_dim)\n",
    "#             torch.save(model, final_snapshot_path)\n",
    "#             for f in glob.glob(final_snapshot_prefix + '*'):\n",
    "#                 if f != final_snapshot_path:\n",
    "#                     os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
